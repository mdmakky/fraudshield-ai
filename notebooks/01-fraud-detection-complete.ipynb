{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981ced15",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Complete Pipeline\n",
    "\n",
    "This notebook demonstrates the full end-to-end pipeline for credit card fraud detection:\n",
    "\n",
    "1. **Data Loading & Exploration**\n",
    "2. **Feature Engineering**\n",
    "3. **Imbalance Handling with SMOTE**\n",
    "4. **Model Training** (Logistic Regression, Random Forest, XGBoost)\n",
    "5. **Model Evaluation & Comparison**\n",
    "6. **Threshold Tuning for High Recall**\n",
    "7. **SHAP Explainability**\n",
    "8. **Results Visualization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00bc773",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3704a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4cd08",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "**Note:** Download the Kaggle Credit Card Fraud dataset and place it in `data/creditcard.csv`\n",
    "\n",
    "Dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823816d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/creditcard.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102bb8b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Fraud cases: {df['Class'].sum():,}\")\n",
    "print(f\"Fraud percentage: {100 * df['Class'].mean():.3f}%\")\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244abbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class imbalance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['Class'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=Legitimate, 1=Fraud)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "labels = ['Legitimate', 'Fraud']\n",
    "sizes = df['Class'].value_counts().values\n",
    "axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.3f%%', startangle=90)\n",
    "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728980c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount and Time analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Amount distribution by class\n",
    "for class_val, color, label in zip([0, 1], ['#2ecc71', '#e74c3c'], ['Legitimate', 'Fraud']):\n",
    "    subset = df[df['Class'] == class_val]['Amount']\n",
    "    axes[0, 0].hist(subset, bins=50, alpha=0.7, label=label, color=color)\n",
    "axes[0, 0].set_xlabel('Amount')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Amount Distribution by Class')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Box plot of Amount\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Amount by Class')\n",
    "axes[0, 1].set_xlabel('Class (0=Legitimate, 1=Fraud)')\n",
    "axes[0, 1].set_ylabel('Amount')\n",
    "\n",
    "# Time distribution\n",
    "for class_val, color, label in zip([0, 1], ['#2ecc71', '#e74c3c'], ['Legitimate', 'Fraud']):\n",
    "    subset = df[df['Class'] == class_val]['Time']\n",
    "    axes[1, 0].hist(subset, bins=50, alpha=0.7, label=label, color=color)\n",
    "axes[1, 0].set_xlabel('Time (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Time Distribution by Class')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Statistics\n",
    "stats = df.groupby('Class')[['Amount', 'Time']].describe()\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.1, 0.5, str(stats), fontsize=10, family='monospace')\n",
    "axes[1, 1].set_title('Statistics by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad3146",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df_eng = df.copy()\n",
    "\n",
    "# Log transform Amount (handles skewness)\n",
    "df_eng['Amount_log'] = np.log1p(df_eng['Amount'])\n",
    "\n",
    "# Extract hour of day\n",
    "df_eng['Hour'] = (df_eng['Time'] // 3600) % 24\n",
    "\n",
    "# Day number\n",
    "df_eng['Day'] = df_eng['Time'] // (3600 * 24)\n",
    "\n",
    "# Amount bins\n",
    "df_eng['Amount_bin'] = pd.qcut(df_eng['Amount'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "print(\"Feature engineering complete!\")\n",
    "print(f\"New shape: {df_eng.shape}\")\n",
    "print(f\"\\nNew features: Amount_log, Hour, Day, Amount_bin\")\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affcc07",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc94f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_eng.drop(columns=['Class'])\n",
    "y = df_eng['Class']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({y_train.sum()} frauds)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({y_test.sum()} frauds)\")\n",
    "\n",
    "# Scale specific features\n",
    "scaler = RobustScaler()\n",
    "cols_to_scale = ['Amount', 'Amount_log', 'Hour', 'Day', 'Amount_bin']\n",
    "\n",
    "scaler.fit(X_train[cols_to_scale])\n",
    "X_train[cols_to_scale] = scaler.transform(X_train[cols_to_scale])\n",
    "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(\"\\nâœ“ Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6013a",
   "metadata": {},
   "source": [
    "## 6. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb93796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"SMOTE Resampling Results:\")\n",
    "print(f\"Original training set: {len(X_train)} samples\")\n",
    "print(f\"  - Legitimate: {(y_train==0).sum()}\")\n",
    "print(f\"  - Fraud: {(y_train==1).sum()}\")\n",
    "print(f\"\\nResampled training set: {len(X_res)} samples\")\n",
    "print(f\"  - Legitimate: {(y_res==0).sum()}\")\n",
    "print(f\"  - Fraud: {(y_res==1).sum()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "y_train.value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "y_res.value_counts().plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ca7f5",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\n[1/3] Training Logistic Regression...\")\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr.fit(X_res, y_res)\n",
    "print(\"âœ“ Logistic Regression trained\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n[2/3] Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_res, y_res)\n",
    "print(\"âœ“ Random Forest trained\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\n[3/3] Training XGBoost...\")\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_clf.fit(X_res, y_res)\n",
    "print(\"âœ“ XGBoost trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79fd1b",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, name='Model', threshold=0.5):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    # Predictions\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name} Evaluation (threshold={threshold})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(classification_report(y, preds, target_names=['Legitimate', 'Fraud']))\n",
    "    \n",
    "    # Metrics\n",
    "    roc_auc = roc_auc_score(y, probs)\n",
    "    pr_auc = average_precision_score(y, probs)\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, preds)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'probs': probs,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "for model, name in [(lr, 'Logistic Regression'), (rf, 'Random Forest'), (xgb_clf, 'XGBoost')]:\n",
    "    result = evaluate_model(model, X_test, y_test, name, threshold=0.5)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7fd36",
   "metadata": {},
   "source": [
    "## 9. Threshold Tuning for High Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdaddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_probs, target_recall=0.90):\n",
    "    \"\"\"Find threshold for target recall.\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    \n",
    "    # Find where recall >= target\n",
    "    indices = np.where(recall >= target_recall)[0]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        return thresholds[0], precision[0], recall[0]\n",
    "    \n",
    "    # Pick highest precision\n",
    "    best_idx = indices[np.argmax(precision[indices])]\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.0\n",
    "    \n",
    "    return best_threshold, precision[best_idx], recall[best_idx]\n",
    "\n",
    "# Find optimal thresholds\n",
    "print(\"=\"*60)\n",
    "print(\"THRESHOLD TUNING FOR 90% RECALL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in results:\n",
    "    thresh, prec, rec = find_optimal_threshold(y_test, result['probs'], target_recall=0.90)\n",
    "    print(f\"\\n{result['name']}:\")\n",
    "    print(f\"  Optimal Threshold: {thresh:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall: {rec:.4f}\")\n",
    "    result['optimal_threshold'] = thresh\n",
    "    result['optimal_precision'] = prec\n",
    "    result['optimal_recall'] = rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addad9d9",
   "metadata": {},
   "source": [
    "## 10. Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "for result in results:\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probs'])\n",
    "    axes[0].plot(fpr, tpr, label=f\"{result['name']} (AUC={result['roc_auc']:.3f})\", linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "for result in results:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, result['probs'])\n",
    "    axes[1].plot(recall, precision, label=f\"{result['name']} (AUC={result['pr_auc']:.3f})\", linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Model': r['name'],\n",
    "    'ROC-AUC': r['roc_auc'],\n",
    "    'PR-AUC': r['pr_auc'],\n",
    "    'Default Threshold': r['threshold'],\n",
    "    'Optimal Threshold': r['optimal_threshold'],\n",
    "    'Optimal Precision': r['optimal_precision'],\n",
    "    'Optimal Recall': r['optimal_recall']\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_model = comparison_df.loc[comparison_df['PR-AUC'].idxmax(), 'Model']\n",
    "print(f\"\\nðŸ† Best Model (by PR-AUC): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345150b",
   "metadata": {},
   "source": [
    "## 11. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c55851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost for SHAP (best model typically)\n",
    "print(\"Generating SHAP explanations for XGBoost...\")\n",
    "\n",
    "# Sample for speed\n",
    "sample_size = 1000\n",
    "X_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.TreeExplainer(xgb_clf)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"âœ“ SHAP values computed for {sample_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26139c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_sample, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1859c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (feature importance)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type='bar', max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f46594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plot for top feature\n",
    "top_feature = feature_importance.iloc[0]['feature']\n",
    "print(f\"Dependence plot for top feature: {top_feature}\")\n",
    "\n",
    "shap.dependence_plot(\n",
    "    top_feature,\n",
    "    shap_values,\n",
    "    X_sample,\n",
    "    interaction_index='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaaa61",
   "metadata": {},
   "source": [
    "## 12. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models and artifacts\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "joblib.dump(lr, '../models/lr.joblib')\n",
    "joblib.dump(rf, '../models/rf.joblib')\n",
    "joblib.dump(xgb_clf, '../models/xgb.joblib')\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('../models/model_comparison.csv', index=False)\n",
    "\n",
    "print(\"âœ“ All models saved to ../models/\")\n",
    "print(\"  - lr.joblib\")\n",
    "print(\"  - rf.joblib\")\n",
    "print(\"  - xgb.joblib\")\n",
    "print(\"  - scaler.joblib\")\n",
    "print(\"  - model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6f0c9",
   "metadata": {},
   "source": [
    "## 13. Summary & Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "- Successfully trained 3 models with SMOTE for class imbalance\n",
    "- XGBoost typically achieves the best PR-AUC score\n",
    "- Optimal threshold tuning achieves 90%+ recall with acceptable precision\n",
    "- SHAP analysis reveals most important features for fraud detection\n",
    "\n",
    "### Next Steps:\n",
    "1. **Deploy**: Use FastAPI service (`src/api.py`) for real-time predictions\n",
    "2. **Monitor**: Track model performance over time, detect data drift\n",
    "3. **Retrain**: Update model monthly or when performance degrades\n",
    "4. **Features**: Add merchant, location, device fingerprint data for better accuracy\n",
    "5. **Ensemble**: Combine multiple models for improved robustness\n",
    "\n",
    "### Production Recommendations:\n",
    "- Set threshold based on business cost of false positives vs false negatives\n",
    "- Implement A/B testing for threshold optimization\n",
    "- Add human-in-the-loop for high-value transactions\n",
    "- Monitor fraud patterns and update features accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0388da",
   "metadata": {},
   "source": [
    "---\n",
    "**Notebook Complete** âœ“\n",
    "\n",
    "Run the FastAPI service:\n",
    "```bash\n",
    "cd ..\n",
    "python -m uvicorn src.api:app --reload\n",
    "```\n",
    "\n",
    "Then test predictions at: http://localhost:8000/docs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
