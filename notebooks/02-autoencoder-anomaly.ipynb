{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9170cefc",
   "metadata": {},
   "source": [
    "# Autoencoder-Based Anomaly Detection for Fraud\n",
    "\n",
    "This notebook demonstrates an **unsupervised approach** to fraud detection using autoencoders:\n",
    "\n",
    "1. **Train on legitimate transactions only** (unsupervised)\n",
    "2. **Detect fraud as anomalies** based on reconstruction error\n",
    "3. **Tune threshold** for optimal detection\n",
    "4. **Compare with supervised methods**\n",
    "\n",
    "## Key Advantage:\n",
    "Autoencoders can detect novel fraud patterns not seen during training!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a008fa",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc830333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf4d9b",
   "metadata": {},
   "source": [
    "## 2. Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/creditcard.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud cases: {df['Class'].sum()} ({100*df['Class'].mean():.3f}%)\")\n",
    "\n",
    "# Feature engineering\n",
    "df['Amount_log'] = np.log1p(df['Amount'])\n",
    "df['Hour'] = (df['Time'] // 3600) % 24\n",
    "df['Day'] = df['Time'] // (3600 * 24)\n",
    "df['Amount_bin'] = pd.qcut(df['Amount'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "print(\"\\n✓ Feature engineering complete\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "cols_to_scale = ['Amount', 'Amount_log', 'Hour', 'Day', 'Amount_bin']\n",
    "\n",
    "scaler.fit(X_train[cols_to_scale])\n",
    "X_train[cols_to_scale] = scaler.transform(X_train[cols_to_scale])\n",
    "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(\"✓ Scaling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899969c",
   "metadata": {},
   "source": [
    "## 3. Extract Legitimate Transactions for Training\n",
    "\n",
    "**Key Insight:** Train autoencoder ONLY on normal (non-fraud) transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only legitimate transactions\n",
    "X_train_normal = X_train[y_train == 0].values\n",
    "X_train_fraud = X_train[y_train == 1].values\n",
    "\n",
    "print(f\"Training on {len(X_train_normal):,} legitimate transactions\")\n",
    "print(f\"Excluding {len(X_train_fraud):,} fraud transactions from training\")\n",
    "\n",
    "# Convert test to numpy\n",
    "X_test_np = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff6b42",
   "metadata": {},
   "source": [
    "## 4. Build Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(n_features, latent_dim=8):\n",
    "    \"\"\"\n",
    "    Build autoencoder with bottleneck architecture.\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Dense(64) -> Dense(32) -> Bottleneck(8) -> Dense(32) -> Dense(64) -> Output\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inp = layers.Input(shape=(n_features,), name='input')\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Dense(64, activation='relu', name='encoder_1')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(32, activation='relu', name='encoder_2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Bottleneck (compressed representation)\n",
    "    encoded = layers.Dense(latent_dim, activation='relu', name='bottleneck')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(32, activation='relu', name='decoder_1')(encoded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', name='decoder_2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer (reconstruct input)\n",
    "    decoded = layers.Dense(n_features, activation='linear', name='output')(x)\n",
    "    \n",
    "    # Build model\n",
    "    autoencoder = models.Model(inp, decoded, name='fraud_autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Build model\n",
    "n_features = X_train_normal.shape[1]\n",
    "autoencoder = build_autoencoder(n_features, latent_dim=8)\n",
    "\n",
    "print(\"Autoencoder Architecture:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c1277",
   "metadata": {},
   "source": [
    "## 5. Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the autoencoder\n",
    "print(\"Training autoencoder on legitimate transactions...\")\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal, X_train_normal,  # Input = Output (reconstruction)\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Training History - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Training History - MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9708064",
   "metadata": {},
   "source": [
    "## 6. Compute Reconstruction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_error(model, X):\n",
    "    \"\"\"Compute MSE reconstruction error for each sample.\"\"\"\n",
    "    reconstructed = model.predict(X, verbose=0)\n",
    "    mse = np.mean(np.square(X - reconstructed), axis=1)\n",
    "    return mse\n",
    "\n",
    "# Compute errors on test set\n",
    "print(\"Computing reconstruction errors...\")\n",
    "test_errors = compute_reconstruction_error(autoencoder, X_test_np)\n",
    "\n",
    "# Separate by class\n",
    "errors_normal = test_errors[y_test == 0]\n",
    "errors_fraud = test_errors[y_test == 1]\n",
    "\n",
    "print(f\"\\nReconstruction Error Statistics:\")\n",
    "print(f\"Legitimate transactions:\")\n",
    "print(f\"  Mean: {errors_normal.mean():.6f}\")\n",
    "print(f\"  Std:  {errors_normal.std():.6f}\")\n",
    "print(f\"  Max:  {errors_normal.max():.6f}\")\n",
    "print(f\"\\nFraud transactions:\")\n",
    "print(f\"  Mean: {errors_fraud.mean():.6f}\")\n",
    "print(f\"  Std:  {errors_fraud.std():.6f}\")\n",
    "print(f\"  Max:  {errors_fraud.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(errors_normal, bins=50, alpha=0.7, label='Legitimate', color='#2ecc71', density=True)\n",
    "axes[0].hist(errors_fraud, bins=50, alpha=0.7, label='Fraud', color='#e74c3c', density=True)\n",
    "axes[0].set_xlabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data = [errors_normal, errors_fraud]\n",
    "bp = axes[1].boxplot(data, labels=['Legitimate', 'Fraud'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('#2ecc71')\n",
    "bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "axes[1].set_ylabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "axes[1].set_title('Reconstruction Error by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b4b39",
   "metadata": {},
   "source": [
    "## 7. Threshold Selection\n",
    "\n",
    "**Goal:** Find threshold that maximizes recall while maintaining acceptable precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702781f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_errors)\n",
    "\n",
    "# Find threshold for 90% recall\n",
    "target_recall = 0.90\n",
    "indices = np.where(recall >= target_recall)[0]\n",
    "\n",
    "if len(indices) > 0:\n",
    "    best_idx = indices[np.argmax(precision[indices])]\n",
    "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else thresholds[0]\n",
    "    optimal_precision = precision[best_idx]\n",
    "    optimal_recall = recall[best_idx]\n",
    "else:\n",
    "    # Fallback: use 95th percentile of normal errors\n",
    "    optimal_threshold = np.percentile(errors_normal, 95)\n",
    "    optimal_precision = precision[0]\n",
    "    optimal_recall = recall[0]\n",
    "\n",
    "print(f\"Optimal Threshold Selection:\")\n",
    "print(f\"  Threshold: {optimal_threshold:.6f}\")\n",
    "print(f\"  Precision: {optimal_precision:.4f}\")\n",
    "print(f\"  Recall: {optimal_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve with optimal threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# PR Curve\n",
    "axes[0].plot(recall, precision, linewidth=2, label='Autoencoder')\n",
    "axes[0].scatter([optimal_recall], [optimal_precision], color='red', s=100, \n",
    "                zorder=5, label=f'Optimal (thresh={optimal_threshold:.4f})')\n",
    "axes[0].set_xlabel('Recall', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Threshold vs Metrics\n",
    "axes[1].plot(thresholds, precision[:-1], label='Precision', linewidth=2)\n",
    "axes[1].plot(thresholds, recall[:-1], label='Recall', linewidth=2)\n",
    "axes[1].axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, label='Optimal Threshold')\n",
    "axes[1].set_xlabel('Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('Precision & Recall vs Threshold', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c25fa",
   "metadata": {},
   "source": [
    "## 8. Evaluate Autoencoder Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab85e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with optimal threshold\n",
    "predictions = (test_errors > optimal_threshold).astype(int)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOENCODER EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, predictions, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTN={tn:,}, FP={fp:,}, FN={fn:,}, TP={tp:,}\")\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test, test_errors)\n",
    "pr_auc = average_precision_score(y_test, test_errors)\n",
    "\n",
    "print(f\"\\nROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Autoencoder', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adcbf8",
   "metadata": {},
   "source": [
    "## 9. Compare with Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd31db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XGBoost model for comparison\n",
    "try:\n",
    "    xgb_model = joblib.load('../models/xgb.joblib')\n",
    "    xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Plot ROC comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Autoencoder ROC\n",
    "    fpr_ae, tpr_ae, _ = roc_curve(y_test, test_errors)\n",
    "    plt.plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC={roc_auc:.3f})', linewidth=2)\n",
    "    \n",
    "    # XGBoost ROC\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs)\n",
    "    roc_auc_xgb = roc_auc_score(y_test, xgb_probs)\n",
    "    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={roc_auc_xgb:.3f})', linewidth=2)\n",
    "    \n",
    "    # Random baseline\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Autoencoder ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"XGBoost ROC-AUC: {roc_auc_xgb:.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"XGBoost model not found. Run 01-fraud-detection-complete.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c686b5",
   "metadata": {},
   "source": [
    "## 10. Analyze Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f03191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find false negatives (missed frauds)\n",
    "false_negatives = np.where((predictions == 0) & (y_test == 1))[0]\n",
    "false_positives = np.where((predictions == 1) & (y_test == 0))[0]\n",
    "\n",
    "print(f\"False Negatives (Missed Frauds): {len(false_negatives)}\")\n",
    "print(f\"False Positives (False Alarms): {len(false_positives)}\")\n",
    "\n",
    "if len(false_negatives) > 0:\n",
    "    fn_errors = test_errors[false_negatives]\n",
    "    print(f\"\\nFalse Negative Errors:\")\n",
    "    print(f\"  Mean: {fn_errors.mean():.6f}\")\n",
    "    print(f\"  Min: {fn_errors.min():.6f}\")\n",
    "    print(f\"  Max: {fn_errors.max():.6f}\")\n",
    "    print(f\"  These frauds have low reconstruction error (look normal)\")\n",
    "\n",
    "if len(false_positives) > 0:\n",
    "    fp_errors = test_errors[false_positives]\n",
    "    print(f\"\\nFalse Positive Errors:\")\n",
    "    print(f\"  Mean: {fp_errors.mean():.6f}\")\n",
    "    print(f\"  Min: {fp_errors.min():.6f}\")\n",
    "    print(f\"  Max: {fp_errors.max():.6f}\")\n",
    "    print(f\"  These normal transactions have high reconstruction error (look unusual)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25c531",
   "metadata": {},
   "source": [
    "## 11. Feature Reconstruction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcf01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which features have highest reconstruction error for frauds\n",
    "reconstructed_test = autoencoder.predict(X_test_np, verbose=0)\n",
    "feature_errors = np.abs(X_test_np - reconstructed_test)\n",
    "\n",
    "# Average error per feature for fraud vs normal\n",
    "fraud_mask = y_test == 1\n",
    "feature_errors_fraud = feature_errors[fraud_mask].mean(axis=0)\n",
    "feature_errors_normal = feature_errors[~fraud_mask].mean(axis=0)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = X_test.columns\n",
    "error_comparison = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Fraud Error': feature_errors_fraud,\n",
    "    'Normal Error': feature_errors_normal,\n",
    "    'Difference': feature_errors_fraud - feature_errors_normal\n",
    "}).sort_values('Difference', ascending=False)\n",
    "\n",
    "print(\"Top 10 Features with Highest Reconstruction Error for Frauds:\")\n",
    "print(error_comparison.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature reconstruction errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_features = error_comparison.head(15)\n",
    "x = np.arange(len(top_features))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, top_features['Normal Error'], width, label='Normal', alpha=0.8, color='#2ecc71')\n",
    "plt.bar(x + width/2, top_features['Fraud Error'], width, label='Fraud', alpha=0.8, color='#e74c3c')\n",
    "\n",
    "plt.xlabel('Feature', fontsize=12)\n",
    "plt.ylabel('Average Reconstruction Error', fontsize=12)\n",
    "plt.title('Feature Reconstruction Errors (Top 15)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, top_features['Feature'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219af51c",
   "metadata": {},
   "source": [
    "## 12. Save Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and threshold\n",
    "autoencoder.save('../models/autoencoder.h5')\n",
    "\n",
    "threshold_info = {\n",
    "    'threshold': optimal_threshold,\n",
    "    'roc_auc': roc_auc,\n",
    "    'pr_auc': pr_auc,\n",
    "    'precision': optimal_precision,\n",
    "    'recall': optimal_recall\n",
    "}\n",
    "joblib.dump(threshold_info, '../models/ae_threshold.joblib')\n",
    "\n",
    "print(\"✓ Autoencoder saved to ../models/autoencoder.h5\")\n",
    "print(\"✓ Threshold info saved to ../models/ae_threshold.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bcbfd",
   "metadata": {},
   "source": [
    "## 13. Summary & Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Advantages of Autoencoder Approach:**\n",
    "- ✅ Unsupervised: doesn't need labeled fraud examples\n",
    "- ✅ Can detect novel fraud patterns not in training data\n",
    "- ✅ Works well when fraud evolves over time\n",
    "- ✅ Useful for highly imbalanced datasets\n",
    "\n",
    "**Limitations:**\n",
    "- ⚠️ Generally lower precision than supervised methods\n",
    "- ⚠️ Threshold tuning can be challenging\n",
    "- ⚠️ May not capture complex fraud patterns as well as tree-based models\n",
    "\n",
    "### Reconstruction Error Insights:\n",
    "- Fraud transactions have **higher reconstruction error** on average\n",
    "- Features that differ most: check `error_comparison` DataFrame above\n",
    "- Some sophisticated frauds may have low error (false negatives)\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Ensemble Approach**: Combine autoencoder with XGBoost\n",
    "   - If EITHER flags as fraud → investigate\n",
    "   - Increases recall, catches more fraud types\n",
    "\n",
    "2. **Threshold Strategy**:\n",
    "   - Use different thresholds for different transaction amounts\n",
    "   - Higher amounts → lower threshold (more sensitive)\n",
    "\n",
    "3. **Continuous Learning**:\n",
    "   - Retrain autoencoder monthly on recent legitimate transactions\n",
    "   - Adapts to changing patterns in normal behavior\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "   - Add merchant category, location, time-of-day features\n",
    "   - These capture behavioral patterns better\n",
    "\n",
    "---\n",
    "**Notebook Complete** ✓"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
